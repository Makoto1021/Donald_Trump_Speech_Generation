{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HGd7-PWmvoIc",
    "outputId": "1dbdb60c-7282-4e07-e9ec-b7c1ff425f8e"
   },
   "outputs": [],
   "source": [
    "# to install pytorch on colab\n",
    "# no need to run on your local\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ROtybUC_vPEv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aZ2OiNtlvPE0",
    "outputId": "5c79a513-af9f-4b27-fa1b-4a459cd02032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114266650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set seed for reproducibility\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B0mbACtnvPE9",
    "outputId": "831c55f9-8912-4090-85e8-8e107536b31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tNqWul88vPFA"
   },
   "outputs": [],
   "source": [
    "#set variables\n",
    "\n",
    "#for batches\n",
    "train_batch_size = 10\n",
    "eval_batch_size = 10\n",
    "\n",
    "#for the model\n",
    "emsize = 200 #size of word embedding\n",
    "nhid = 200 #number of hidden units per layer\n",
    "nlayers = 2 #number of layers\n",
    "dropout = 0.5 #dropout\n",
    "lr = 20 #initial learning rate\n",
    "clip = 0.25 #gradient clipping\n",
    "epochs = 5 #number of epochs\n",
    "log_interval = 200\n",
    "tied = True\n",
    "bptt = 35 #sequence length\n",
    "export_path_model = 'Trump_NLG_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GNtH4mavPFD"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ida_KQIGvPFE"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        # self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HPDPlQgnvPFG"
   },
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "_-0xiXw_vPFI",
    "outputId": "ce0d6e97-fea3-45a9-cffb-88921cb56e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-10 12:52:39--  https://www.dropbox.com/s/n71az5lgaz7xt8v/Trump_NLG.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/n71az5lgaz7xt8v/Trump_NLG.zip [following]\n",
      "--2018-11-10 12:52:39--  https://www.dropbox.com/s/raw/n71az5lgaz7xt8v/Trump_NLG.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com/cd/0/inline/AVO5r7eHllXP8xwaqxWCBA6wtsADbg1HRnxO0-IWprEFhotDOqP7ahdBPSAe1Kx1Uxa7YJfaWPP_UUpnfyzX_wLRoRGW5KcLb88u3v5BgahPd4iSFH8yHbjJH_49TRGe_IFEPtDUsfSLH6BQIYl9hGzFoayZGcX_qNOOU9FxuGO7zoqPqdggHBFBmUmGUTGp31U/file [following]\n",
      "--2018-11-10 12:52:39--  https://uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com/cd/0/inline/AVO5r7eHllXP8xwaqxWCBA6wtsADbg1HRnxO0-IWprEFhotDOqP7ahdBPSAe1Kx1Uxa7YJfaWPP_UUpnfyzX_wLRoRGW5KcLb88u3v5BgahPd4iSFH8yHbjJH_49TRGe_IFEPtDUsfSLH6BQIYl9hGzFoayZGcX_qNOOU9FxuGO7zoqPqdggHBFBmUmGUTGp31U/file\n",
      "Resolving uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com (uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com)... 162.125.67.6\n",
      "Connecting to uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com (uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com)|162.125.67.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/AVPmnSVi75YB4pcm9jugD5G1qmD2tGBXtw7j6dyOk2z8XXRU57Yd84yoytPh4Mn-yLPFW33qAUtYeAqcPqSWTvUsNMBdESgZ6eIXPm-kyvDBCMOOdQcrIh-icaWOmD1MZfs1BSx59BjnWaJYXp5pSiCAbdpxJqfi2taDO9dH3ciwSIWgCcv2T6gYxaShQyCHshX0TzwuhladyAEY7AbC-cEF_xcEAjuo0QWkob5KHbGhkza_Nz6SfKFtvNFaOvUxYdb5V6Jq_TC9VUk_3kFYT2Lps-SyL9NZz_7el4DmoWt09AV6BTHkODTKNdh9xs1qEJBZGo7nLiY-lg2IgDSbyqWULN5s9GAZCjBz_WoxlQOYsYMbj0Y_iLb4C1mhYBzTy9ldXXS9elkbjPqueQ_YNIYCkcUtKXfBJbPUQVSSy2NEbqZrTZtCQ1sllHWjN0U1S_A/file [following]\n",
      "--2018-11-10 12:52:40--  https://uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com/cd/0/inline2/AVPmnSVi75YB4pcm9jugD5G1qmD2tGBXtw7j6dyOk2z8XXRU57Yd84yoytPh4Mn-yLPFW33qAUtYeAqcPqSWTvUsNMBdESgZ6eIXPm-kyvDBCMOOdQcrIh-icaWOmD1MZfs1BSx59BjnWaJYXp5pSiCAbdpxJqfi2taDO9dH3ciwSIWgCcv2T6gYxaShQyCHshX0TzwuhladyAEY7AbC-cEF_xcEAjuo0QWkob5KHbGhkza_Nz6SfKFtvNFaOvUxYdb5V6Jq_TC9VUk_3kFYT2Lps-SyL9NZz_7el4DmoWt09AV6BTHkODTKNdh9xs1qEJBZGo7nLiY-lg2IgDSbyqWULN5s9GAZCjBz_WoxlQOYsYMbj0Y_iLb4C1mhYBzTy9ldXXS9elkbjPqueQ_YNIYCkcUtKXfBJbPUQVSSy2NEbqZrTZtCQ1sllHWjN0U1S_A/file\n",
      "Reusing existing connection to uc3cddd5be04739aa86848581894.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 746381 (729K) [application/zip]\n",
      "Saving to: ‘Trump_NLG.zip’\n",
      "\n",
      "Trump_NLG.zip       100%[===================>] 728.89K   141KB/s    in 5.1s    \n",
      "\n",
      "2018-11-10 12:52:46 (143 KB/s) - ‘Trump_NLG.zip’ saved [746381/746381]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to download the data \n",
    "#put it in a folder with test.txt, train.txt and valid.txt\n",
    "# ! wget https://www.dropbox.com/s/mrtpbpohnrsl3hc/data.zip\n",
    "# ! wget https://www.dropbox.com/s/5hsrba4led6mzjw/Trump_NLG.zip\n",
    "# ! wget https://www.dropbox.com/s/5hsrba4led6mzjw/Trump_NLG.zip\n",
    "! wget https://www.dropbox.com/s/n71az5lgaz7xt8v/Trump_NLG.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "NnVCObyayl7j",
    "outputId": "6eddd544-b2bb-4af0-f59d-370e02040ca0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  Trump_NLG.zip\r\n",
      "  inflating: train.txt               \r\n",
      "   creating: __MACOSX/\r\n",
      "  inflating: __MACOSX/._train.txt    \r\n",
      "  inflating: valid.txt               \r\n",
      "  inflating: __MACOSX/._valid.txt    \r\n"
     ]
    }
   ],
   "source": [
    "! unzip Trump_NLG.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YE8cE9PfvPFL"
   },
   "outputs": [],
   "source": [
    "data_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vFka7JomvPFN"
   },
   "outputs": [],
   "source": [
    "#potentiellement, virer les mots rares ?\n",
    "#commencer avec un embedding déjà existant (gloVe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBVk948uvPFQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PPcfEtzzvPFR"
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3CFUXDKbvPFa"
   },
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, train_batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "# test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, emsize) # map each word into a vector space represented by a dense vector\n",
    "                                                    # emsize  is a size of word embedding\n",
    "        self.rnn = getattr(nn, \"LSTM\")(emsize, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        \n",
    "        if tie_weights:\n",
    "            if nhid != emsize:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout, tied).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt): #iterate other batches\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(train_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(export_path_model, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(export_path_model, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "    \n",
    "#commenting this because here, we don't care much about the test loss. We just want to generate some text.\n",
    "# Run on test data.\n",
    "#test_loss = evaluate(test_data)\n",
    "#print('=' * 89)\n",
    "#print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    " #   test_loss, math.exp(test_loss)))\n",
    "#print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-10 13:18:57--  https://www.dropbox.com/s/ne6osopwbc60u1z/Trump_NLG_model.pt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/ne6osopwbc60u1z/Trump_NLG_model.pt [following]\n",
      "--2018-11-10 13:18:58--  https://www.dropbox.com/s/raw/ne6osopwbc60u1z/Trump_NLG_model.pt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com/cd/0/inline/AVMCJsHHnetEPrmaqHdNVJWiHaWYCoMoI_MzYyREmUJGY5XH9VA87jpytV-lBpNnhtUNh317pLj9LLmgJNFVCj-yIwl9bFpRo5DEb4MRwtEVfMWLL5rYalQ1v94VV8a2zZlhc9UWdTDGmVnMseo2nhRm1WvLQ1s0gsjoBltRtQdER9PCqIPnr6F0oPKSapqqT6E/file [following]\n",
      "--2018-11-10 13:18:58--  https://ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com/cd/0/inline/AVMCJsHHnetEPrmaqHdNVJWiHaWYCoMoI_MzYyREmUJGY5XH9VA87jpytV-lBpNnhtUNh317pLj9LLmgJNFVCj-yIwl9bFpRo5DEb4MRwtEVfMWLL5rYalQ1v94VV8a2zZlhc9UWdTDGmVnMseo2nhRm1WvLQ1s0gsjoBltRtQdER9PCqIPnr6F0oPKSapqqT6E/file\n",
      "Resolving ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com (ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com)... 162.125.67.6\n",
      "Connecting to ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com (ucc29c959fefa88260e4bb83ec9f.dl.dropboxusercontent.com)|162.125.67.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/plain]\n",
      "Saving to: ‘Trump_NLG_model.pt’\n",
      "\n",
      "Trump_NLG_model.pt      [  <=>               ]  16.48M  1.14MB/s    in 15s     \n",
      "\n",
      "2018-11-10 13:19:15 (1.08 MB/s) - ‘Trump_NLG_model.pt’ saved [18543948]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if needed, download model.pt\n",
    "# ! wget https://www.dropbox.com/s/s5dp5rse2by67jy/model.pt\n",
    "! wget https://www.dropbox.com/s/ne6osopwbc60u1z/Trump_NLG_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/500 words\n",
      "| Generated 200/500 words\n",
      "| Generated 400/500 words\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.8\n",
    "numberwords = 500\n",
    "# temperature has to be greater or equal 1e-3\n",
    "\n",
    "with open('Trump_NLG_model.pt', 'rb') as f:\n",
    "    model = torch.load(f, map_location='cpu').to(device)\n",
    "    # model = torch.load(f).to(device) # when running on gpu\n",
    "model.eval()\n",
    "\n",
    "#set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "with open('./text_generated.txt', 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(numberwords):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, numberwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and all over the United States. And believe me, the reason they want to -- they say we have a\r\n",
      "disaster. And it is a disaster. <APPLAUSE> The U.K., of the global speech. The Paris is a beauty. The lot\r\n",
      "of people -- her by far. So you've always read Coach fund in his statements that think it's a long\r\n",
      "time and I'm trying to have a lot of people out of work. We are going to make America strong\r\n",
      "again, we will make America strong again, we will make America safe again. We will make America great again. Thank\r\n",
      "you. Thank you. Thank you, everybody. Thank you. God bless you. Thank you. Thank you vote, thank you for being\r\n",
      "very rich again. Thank you, everybody. Thank you, everybody. God bless you, everybody. God bless you everybody. Thank you. <title=\"Donald\r\n",
      "Trump, Republican Presidential Candidate, delivers remarks at a campaign event in West Bend, Bay, (SD)\"> <eos> <date:\"2016-10-06\"> <eos> <TRUMP:> Thank\r\n",
      "you very much. Thank you very much. (APPLAUSE) God bless you very much. Thank you. Thank you, everybody. Thank you.\r\n",
      "<APPLAUSE> Thank you. God bless you, everybody. Thank you very much. Thank you. Thank you very much, everybody. Thank you\r\n",
      "very much. Thank you very much. Thank you very much. <APPLAUSE> Thank you. Thank you very much. <APPLAUSE> <eos> <AUDIENCE:\r\n",
      "USA! USA! USA! USA!> <eos> <TRUMP:> We are going to restore the crippling oppression of the United States? <APPLAUSE> Take\r\n",
      "a look at this end on the United States, and this military is any more as NAFTA. This is a\r\n",
      "movement like they've never seen anything like this. <APPLAUSE> In fact, we're going to take care of our vets. Our\r\n",
      "jobs are being poisoned over the country for the United States, we're going to hire our people with our country\r\n",
      "again. And we're going to get rid of Common Core. We're going to take care of our vets. If I\r\n",
      "want to thank you very much. <APPLAUSE> I can tell you. I will make sure that will be one of\r\n",
      "the most important issues in our country. <APPLAUSE> And we're going to bring back our jobs to our country. We're\r\n",
      "going to win for every American family and other places. We're going to have your entire communities and your steelworkers\r\n",
      "to come back to North Carolina and our country is going to be consequences. It's going to be done. You're\r\n",
      "going to end it? We're going to make them strong again. We have to have a future of, by and\r\n",
      "in the world. <APPLAUSE> But we will make America strong again. We will make America strong again. <APPLAUSE> We will\r\n",
      "make America safe again, we will make America great again. Thank you very much. <APPLAUSE> Thank you. Thank you. Thank\r\n",
      "you very much. Thank you. Thank you. Thank you. Thank you. Thank you. <APPLAUSE> Thank you. Thank you, everybody. Thank\r\n",
      "you very much. Thank you. Thank you. Thank you. God bless you. Thank you. Thank you. Thank you.<title=\"Donald Trump, Republican\r\n"
     ]
    }
   ],
   "source": [
    "!cat text_generated.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete files to retrain and generate something again\n",
    "! rm Trump_NLG_model.pt\n",
    "#! rm text_generated.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1c9bcfdc8ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 1. Authenticate and create the PyDrive client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auth' is not defined"
     ]
    }
   ],
   "source": [
    "# use when running on google colab\n",
    "\"\"\"\n",
    "# save those on Drive\n",
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "upload = drive.CreateFile({'title': 'Trump_NLG_model.pt'})\n",
    "upload.SetContentFile('Trump_NLG_model.pt')\n",
    "upload.Upload()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "upload = drive.CreateFile({'title': 'text_generated.txt'})\n",
    "upload.SetContentFile('text_generated.txt')\n",
    "upload.Upload()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
